# mlx-examples-llm-llama Timings

***This is based on [mlx-explore / mlx-examples/llm/llama](https://github.com/ml-explore/mlx-examples.git)***

Please consult the original README.md for setup information

`llama_perf.py` is based on the original `llama.py` and prints timings, to compare MLX llama-2 inference performance (fp16 and quantized - use corresponding models) with the [llama.cpp Apple Silicon Benchmark Results](https://github.com/ggerganov/llama.cpp/discussions/4167)
